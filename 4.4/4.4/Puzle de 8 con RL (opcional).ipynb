{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea81f93",
   "metadata": {},
   "source": [
    " ## Puzle de 8 con RL\n",
    " \n",
    "Se aplica el algoritmo Q-learning para diseñar un agente que aprenda a resolver el puzzle del 8. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78d00e",
   "metadata": {},
   "source": [
    "Para poder aplicar Q-learning necesitamos numerar las diferentes acciones y estados para poder crear la q_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5622145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a827a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import Problem\n",
    "import itertools\n",
    "class Ocho_Puzzle(Problem):\n",
    "    \"\"\"Problema a del 8-puzzle.  Los estados serán tuplas de nueve elementos,\n",
    "    permutaciones de los números del 0 al 8 (el 0 es el hueco). Representan la\n",
    "    disposición de las fichas en el tablero, leídas por filas de arriba a\n",
    "    abajo, y dentro de cada fila, de izquierda a derecha. Las cuatro\n",
    "    acciones del problema las representaremos mediante las cadenas:\n",
    "    \"Mover hueco arriba\", \"Mover hueco abajo\", \"Mover hueco izquierda\" y\n",
    "    \"Mover hueco derecha\", respectivamente.\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, initial, goal=(1, 2, 3, 4, 5, 6, 7, 8, 0)):\n",
    "        \"\"\" Define goal state and initialize a problem \"\"\"\n",
    "        self.goal = goal\n",
    "        Problem.__init__(self, initial, goal)\n",
    "        \n",
    "    \n",
    "    def actions(self,estado):\n",
    "     \n",
    "        pos_hueco=estado.index(0) # busco la posicion del 0\n",
    "        accs=list()\n",
    "        if pos_hueco not in (0,1,2):\n",
    "            accs.append(\"Mover hueco arriba\")\n",
    "        if pos_hueco not in (2,5,8):\n",
    "            accs.append(\"Mover hueco derecha\")\n",
    "        if pos_hueco not in (0,3,6): \n",
    "            accs.append(\"Mover hueco izquierda\") \n",
    "        if pos_hueco not in (6,7,8):\n",
    "            accs.append(\"Mover hueco abajo\")\n",
    "                \n",
    "        return accs     \n",
    "\n",
    "    def result(self,estado,accion):\n",
    "        pos_hueco = estado.index(0)\n",
    "        l = list(estado)\n",
    "        if accion == \"Mover hueco arriba\":\n",
    "            l[pos_hueco] = l[pos_hueco-3]\n",
    "            l[pos_hueco-3] = 0\n",
    "        if accion == \"Mover hueco abajo\":\n",
    "            l[pos_hueco] = l[pos_hueco + 3]\n",
    "            l[pos_hueco + 3] = 0\n",
    "        if accion == \"Mover hueco derecha\":\n",
    "            l[pos_hueco] = l[pos_hueco+1]\n",
    "            l[pos_hueco+1] = 0\n",
    "        if accion == \"Mover hueco izquierda\":\n",
    "            l[pos_hueco] = l[pos_hueco-1]\n",
    "            l[pos_hueco-1] = 0\n",
    "        return tuple(l)\n",
    "    \n",
    "    def h(self, node):\n",
    "        \"\"\" Return the heuristic value for a given state. \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def check_solvability(self, state):\n",
    "        \"\"\" Checks if the given state is solvable \"\"\"\n",
    "    # The solvability of a configuration can be checked by calculating the Inversion Permutation. \n",
    "    #If the total Inversion Permutation is even then the initial configuration is solvable else the initial configuration is not solvable which means that only 9!/2 initial states lead to a solution.\n",
    "        inversion = 0\n",
    "        for i in range(len(state)):\n",
    "            for j in range(i+1, len(state)):\n",
    "                if (state[i] > state[j]) and state[i] != 0 and state[j]!= 0:\n",
    "                    inversion += 1\n",
    "        return inversion % 2 == 0\n",
    "    \n",
    "    def lista_estados(self,estado):\n",
    "        i = 0;\n",
    "        diccionario = {}\n",
    "        for e in itertools.permutations(estado):\n",
    "            diccionario[e] = i\n",
    "            i = i + 1;\n",
    "        return diccionario  \n",
    "    \n",
    "    def listaAcciones(self):\n",
    "        listaAcciones = {}\n",
    "        listaAcciones[0] =\"Mover hueco arriba\" \n",
    "        listaAcciones[1] =\"Mover hueco abajo\"\n",
    "        listaAcciones[2] =\"Mover hueco derecha\"\n",
    "        listaAcciones[3] =\"Mover hueco izquierda\"\n",
    "        return listaAcciones\n",
    "    \n",
    "    def listaAcciones2(self):\n",
    "        listaAcciones2 = {}\n",
    "        listaAcciones2[\"Mover hueco arriba\"] = 0\n",
    "        listaAcciones2[\"Mover hueco abajo\"] =1\n",
    "        listaAcciones2[\"Mover hueco derecha\"] =2\n",
    "        listaAcciones2[\"Mover hueco izquierda\"] =3\n",
    "        return listaAcciones2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ec5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos e inicializamos la Q-Table\n",
    "totalAcciones = 4\n",
    "totalEstados = math.factorial(9)\n",
    "q_table = np.zeros([totalEstados, totalAcciones])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3245eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial=(1,2,0,4,5,3,7,8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba158298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estado objetivo (1, 2, 3, 4, 5, 6, 7, 8, 0)\n"
     ]
    }
   ],
   "source": [
    "ocho = Ocho_Puzzle(initial)\n",
    "print(\"estado objetivo\",ocho.goal)   \n",
    "ocho.__init__(initial) \n",
    "state = ocho.initial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88bbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompensa(state):\n",
    "    if state==ocho.goal:\n",
    "        return 100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c18146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agente_puzle8(episodios=10000, alpha = 0.7, gamma = 0.6, epsilon = 0.8):\n",
    "    \"\"\"Creamos y entrenamos el agente con los parametros dados\"\"\"\n",
    "    state = ocho.initial\n",
    "    listaAcciones2 = ocho.listaAcciones2()\n",
    "    listaAcciones = ocho.listaAcciones()\n",
    "    listaEstados = ocho.lista_estados(state)\n",
    "    exitos=0\n",
    "    for i in range(1, episodios):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            mejor = listaAcciones2[random.choice(ocho.actions(state))]\n",
    "            #print(\"La accion aleatoria seleccionada es \", listaAcciones[mejor])\n",
    "        else:\n",
    "            mejor = 0\n",
    "            for accion in listaAcciones.keys():\n",
    "                if q_table[listaEstados[state]][accion] > q_table[listaEstados[state]][mejor]:\n",
    "                    mejor = accion\n",
    "                    #print(\"La mejor accion es \", listaAcciones[mejor])\n",
    "\n",
    "        if (listaAcciones[mejor] in ocho.actions(state)):\n",
    "            sig_estado = ocho.result(state , listaAcciones[mejor])\n",
    "            reward = recompensa(sig_estado)\n",
    "        else:\n",
    "            reward = -50 #accion no aplicable\n",
    "            sig_estado =state\n",
    "        old_value = q_table[listaEstados[state]][mejor]\n",
    "        next_max = np.max(q_table[listaEstados[sig_estado]][mejor])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[listaEstados[state], mejor] = new_value\n",
    "        state = sig_estado\n",
    "        if state == ocho.goal:\n",
    "            #Termino el episodio con éxito. Se ha encontrado la solución\n",
    "            state = ocho.initial\n",
    "            exitos=exitos+1\n",
    "            continue\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "\n",
    "    print(\"Training finished.\\n\") \n",
    "    print(\"La Q-table:\\n\") \n",
    "    print(q_table)\n",
    "    # Métrica del % de veces que ha llegado al estado objetivo. \n",
    "    print(\"Número de éxitos: \", exitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf75046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFICA LOS PARÁMETROS Y OBSERVA EL APRENDIZAJE\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.8\n",
    "episodios=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "272c3933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 99900\n",
      "Training finished.\n",
      "\n",
      "La Q-table:\n",
      "\n",
      "[[-25.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " ...\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.]]\n",
      "Número de éxitos:  1\n"
     ]
    }
   ],
   "source": [
    "# puedes medir el tiempo de aprendizaje \n",
    "#%%time\n",
    "agente_puzle8(episodios,alpha,gamma,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "917e100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_agente_puzle8(initial):\n",
    "    \"\"\"Solo explotación del agente sin modificar la tabla Q\"\"\"\n",
    "    exitos=0\n",
    "    fallos=0\n",
    "    state = initial\n",
    "    episodios =100\n",
    "    #listaAcciones2 = ocho.listaAcciones2()\n",
    "    listaAcciones = ocho.listaAcciones()\n",
    "    listaEstados = ocho.lista_estados(state)\n",
    "    for i in range(1, episodios+1):\n",
    "        done =False\n",
    "        while not done:\n",
    "            mejor_accion = 0        \n",
    "            for accion in listaAcciones.keys():\n",
    "                if q_table[listaEstados[state]][accion] > q_table[listaEstados[state]][mejor_accion]:\n",
    "                    mejor_accion = accion                \n",
    "            print(\"La accion aleatoria seleccionada es \", listaAcciones[mejor_accion])\n",
    "            print(\"Estado:\", state)\n",
    "            if (listaAcciones[mejor_accion] in ocho.actions(state)):\n",
    "                state = ocho.result(state , listaAcciones[mejor_accion])\n",
    "                print(\"Estado siguiente:\", state)\n",
    "            else: \n",
    "                #termino con fallo porque la accion no es aplicable\n",
    "                fallos=fallos+1\n",
    "                done = True            \n",
    "            if state == ocho.goal:\n",
    "                #print(\"Termino el episodio con éxito. Se ha encontrado la solución\")\n",
    "                exitos=exitos+1 \n",
    "                done =True \n",
    "                state=initial\n",
    "            if i % 100 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Episodio: {i}\")\n",
    "        \n",
    "    print(\"Evaluación terminada.\\n\") \n",
    "    # Métrica del % de veces que ha llegado al estado objetivo. \n",
    "    print(\"Número de éxitos:\", exitos)\n",
    "    print(\"Número de éxitos:\", (exitos/episodios)*100, \" %\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55fc740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 100\n",
      "Evaluación terminada.\n",
      "\n",
      "Número de éxitos: 100\n",
      "Número de éxitos: 100.0  %\n"
     ]
    }
   ],
   "source": [
    "evalua_agente_puzle8(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23daea-171b-44d7-a43f-78e920628b62",
   "metadata": {},
   "source": [
    "¿Funciona bien el entrenamiento?\n",
    "¿Puedes mejorarlo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a640bd-a67e-4110-9a0a-b717c5d162a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estado objetivo (1, 2, 3, 4, 5, 6, 7, 8, 0)\n"
     ]
    }
   ],
   "source": [
    "initial2=(1,5,7,4,2,3,0,8,6)\n",
    "ocho2 = Ocho_Puzzle(initial2)\n",
    "print(\"estado objetivo\",ocho.goal)   \n",
    "ocho2.__init__(initial) \n",
    "state = ocho2.initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477321f",
   "metadata": {},
   "source": [
    "El agente ha aprendido con QLearning a resolver el problema del 8 puzle empezando en un estado inicial fijo y ha tardado un cierto tiempo.\n",
    "Una vez entrenado puedes ver qué tal se comporta (usando sólo la parte de explotación de la matriz Q aprendida y midiendo el porcentaje de éxitos). \n",
    "\n",
    "#### Responde a las siguientes cuestiones:\n",
    "\n",
    "- Justifica de forma razonada si esta aproximación con aprendizaje es mejor que la aproximación con búsqueda heurística de la primera práctica.\n",
    "- Observa la matriz de aprendizaje ¿qué ha aprendido el agente?\n",
    "- Si entrenas el agente para un estado inicial ¿cómo se comporta al evaluarlo con otro estado inicial? \n",
    "- Realiza los cambios necesarios para que el agente aprenda a resolver el problema desde cualquier estado inicial.\n",
    "- Te parece adecuada la función de recompensa utilizada. ¿Crees que se podría mejorar el rendimiento del agente cambiando la recompensa?\n",
    "- Comprueba cómo se comporta el aprendizaje con distintas configuraciones de los parámetros. Indica claramente cuál es la mejor configuración que has encontrado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31e29c-7312-422f-ad74-4500d14b4de7",
   "metadata": {},
   "source": [
    "1. Es mejor la forma con RL debido a que, a pesar de que al principio pueda ser más costoso, el resto de veces que lo ejecutas (tras entrenarlo) el coste es mucho mejor debido a que ya sabe cuales son las mejores opciones en cada estado. Sin embargo, aunque la versión heurística pueda parecer menos costosa en la primera ejecución, no hay ninguna mejora en el coste debido a que no aprende nada. Por lo tanto, a la larga es mejor RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f1535e-4cad-43bf-a0b7-a120f2c4a964",
   "metadata": {},
   "source": [
    "2. Observando la matriz de aprendizaje, vemos que hay varias casillas que tienen valores negativos. Esto quiere decir que escogiendo esa opción no vamos a llegar a una solución válida sin pasar por movimientos \"ilegales\". Sin embargo, aunque en las filas mostradas de la Q-tabla al hacer print no veamos valores cercanos al 100 (debido a que esta \"porción\" de tabla no es la tabla entera) sabemos que en el resto de la tabla tiene que haber posiciones que tengan necesariamente esos valores, debido a que sí que hay estados prometedores. No siempre vemos -50 o 100 debido a los parámetros seleccionados (alpha, gamma y epsilon), sino que se van \"relajando\" por la fórmula aplicada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1a10d-fd52-44a7-9034-0db62ec0de1a",
   "metadata": {},
   "source": [
    "3. Al evaluar el agente con el estado inicial2, habiendo sido entrenado con el estado inicial (inicial!=inicial2), observamos que en ninguna de las 100 ejecuciones tiene éxito. Esto se debe a que el agente ha sido entrenado con un único estado inicial en todos los episodios, lo cual lo \"prepara\" solamente para una cantidad de estados reducida. Por tanto, al cambiar el estado inicial para evaluarlo no sabe cómo resolver el problema al aparecer estados que nunca había visto antes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a50f69-5774-4edf-a595-50b421a44020",
   "metadata": {},
   "source": [
    "4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7f1f9a3-36bf-4540-badb-0c825c1948a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 100\n",
      "Evaluación terminada.\n",
      "\n",
      "Número de éxitos: 100\n",
      "Número de éxitos: 100.0  %\n"
     ]
    }
   ],
   "source": [
    "#Hacemos un random de los estados iniciales para que entrene con varios estados iniciales y así la q-table esté rellena por lugares que probando con un solo estado esté vacía.\n",
    "evalua_agente_puzle8(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d0b2b-1d14-4f9f-bbc1-82c13a7e16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1, 1000):\n",
    "    list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n",
    "    initial = tuple(random.sample(list1, 10))\n",
    "    ocho = Ocho_Puzzle(initial)\n",
    "    ocho.__init__(initial)\n",
    "    state = ocho.initial\n",
    "    if(ocho.check_solvability(initial)):\n",
    "        alpha = 0.5\n",
    "        gamma = 0.9\n",
    "        epsilon = 0.8\n",
    "        episodios=100\n",
    "        agente_puzle8(episodios,alpha,gamma,epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c64ca3-f50b-46d7-9962-19cbca318a62",
   "metadata": {},
   "source": [
    "Para que funcione óptimamente desde cualquier estado inicial, lo que podemos hacer es llamar al entrenamiento desde varios estados iniciales (generados aleatoriamente) para que se rellene la q_table en todas las posiciones, ya que si solo probamos desde un único estado inicial, solo se rellena una parte reducida de la tabla, y si lo queremos aplicar a un estado inicial bastante diferente, es posible que no esté entrenado para él. Para que no de error, antes del llamar al agente entrenador, comprobamos que el estado aleatorio inicial que se ha creado sea resoluble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e75f68-30ea-4619-846a-95e58aedc550",
   "metadata": {},
   "source": [
    "5. Sí, consideramos que la recompensa utilizada es adecuada. Sin embargo, creemos que se podría mejorar añadiendo, a parte de la recompensa de 100 cuando se alcanza el estado final, una penalización por cada paso que se toma (para intentar llegar así en el menor número de pasos al puzle resuelto) y una recompensa por cada número que esté en su posición objetivo. De esta forma, aumentaremos la recompensa a medida que reducimos la \"distancia\" al estado objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f590a4-cba0-4dea-b00e-0ec338550340",
   "metadata": {},
   "source": [
    "6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7614ded-cde2-4559-ace7-56a534367ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 100\n",
      "Evaluación terminada.\n",
      "\n",
      "Número de éxitos: 100\n",
      "Número de éxitos: 100.0  %\n"
     ]
    }
   ],
   "source": [
    "# creamos e inicializamos la Q-Table\n",
    "totalAcciones = 4\n",
    "totalEstados = math.factorial(9)\n",
    "q_table = np.zeros([totalEstados, totalAcciones])   \n",
    "initial=(1,2,0,4,5,3,7,8,6)\n",
    "ocho2 = Ocho_Puzzle(initial)\n",
    "print(\"estado objetivo\",ocho.goal)   \n",
    "ocho2.__init__(initial) \n",
    "state = ocho2.initial\n",
    "alpha = 0.1\n",
    "gamma = 0.1\n",
    "epsilon = 0.1\n",
    "episodios=5\n",
    "agente_puzle8(episodios,alpha,gamma,epsilon)\n",
    "evalua_agente_puzle8(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98d37e6b-af76-4489-abd7-27aab4a609ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 100\n",
      "Evaluación terminada.\n",
      "\n",
      "Número de éxitos: 0\n",
      "Número de éxitos: 0.0  %\n"
     ]
    }
   ],
   "source": [
    "# creamos e inicializamos la Q-Table\n",
    "totalAcciones = 4\n",
    "totalEstados = math.factorial(9)\n",
    "q_table = np.zeros([totalEstados, totalAcciones])   \n",
    "initial=(1,2,0,4,5,3,7,8,6)\n",
    "ocho2 = Ocho_Puzzle(initial)\n",
    "print(\"estado objetivo\",ocho.goal)   \n",
    "ocho2.__init__(initial) \n",
    "state = ocho2.initial\n",
    "alpha = 0.9\n",
    "gamma = 0.9\n",
    "epsilon = 0.9\n",
    "episodios=5\n",
    "agente_puzle8(episodios,alpha,gamma,epsilon)\n",
    "evalua_agente_puzle8(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "efc34571-a40e-4e83-a44e-9a2e7e123502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 100\n",
      "Evaluación terminada.\n",
      "\n",
      "Número de éxitos: 100\n",
      "Número de éxitos: 100.0  %\n"
     ]
    }
   ],
   "source": [
    "# creamos e inicializamos la Q-Table\n",
    "totalAcciones = 4\n",
    "totalEstados = math.factorial(9)\n",
    "q_table = np.zeros([totalEstados, totalAcciones])   \n",
    "initial=(1,2,0,4,5,3,7,8,6)\n",
    "ocho2 = Ocho_Puzzle(initial)\n",
    "print(\"estado objetivo\",ocho.goal)   \n",
    "ocho2.__init__(initial) \n",
    "state = ocho2.initial\n",
    "alpha = 0.5\n",
    "gamma = 0.5\n",
    "epsilon = 0.5\n",
    "episodios=5\n",
    "agente_puzle8(episodios,alpha,gamma,epsilon)\n",
    "evalua_agente_puzle8(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe6c1c-05a9-45fc-a560-d15f661430be",
   "metadata": {},
   "source": [
    "Hemos probado con bastantes combinaciones de parámetros (dejamos aquí reflejadas tres de nuestras pruebas) y hemos llegado a las siguientes conclusiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03279d6e-9860-4e1c-8224-3aa4783bd4f2",
   "metadata": {},
   "source": [
    "Hemos observado que con un número elevado de episodios de entrenamientos, siempre funciona mientras los parámetros sean estrictamente mayores que 0 o estrictamente menores que 1.\n",
    "Ya una vez reducido el número de episodios de entrenamiento, podemos ver que poniendo todos los parámetros a 0.1 y vemos que funciona igualmente, mientras que poniéndolos todos a 0.9, deja de funcionar. Evidentemente, si funciona con los parámetros a 0.1, también va a funcionar con ellos inicializados a 0.5.\n",
    "Además, hemos visto que lo óptimo es ir reduciendo dinámicamente todos los parámetros, pero para ello, necesitaríamos más episodios de entrenamiento, por ello, en este problema que es sencillo, no vamos a notar la diferencia.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
