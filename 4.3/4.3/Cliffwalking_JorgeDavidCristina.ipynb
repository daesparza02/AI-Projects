{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d59470-ea0e-4b99-8413-52621fc90319",
   "metadata": {},
   "source": [
    "Jorge Arboleya Carrio, David Esparza Sainz, Cristina Gómez Calvo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7d05d-e016-4a36-a45b-e8518e41a391",
   "metadata": {},
   "source": [
    "### Ejemplo 3. CliffWalking  (para entregar) \n",
    "\n",
    "Para esta práctica se pide entrenar y analizar en detalle el comportamiento de un agente para el problema CliffWalking (y/o BlackJack).\n",
    "Puedes encontrar más información en la documentación de Gymnasium:\n",
    "[https://gymnasium.farama.org/environments/toy_text/cliff_walking/]\n",
    "\n",
    "El juego comienza con el jugador en la ubicación (3, 0) de una cuadrícula de 4x12, con el objetivo situado en (3, 11). Si el jugador llega al objetivo, el episodio termina.\n",
    "Un precipicio se extiende a lo largo de (3, 1..10). Si el jugador se mueve a una ubicación del precipicio, vuelve al punto de inicio.\n",
    "\n",
    "El jugador realiza movimientos hasta llegar al objetivo.\n",
    "\n",
    "Adaptado del Ejemplo 6.6 (página 132) del libro Reinforcement Learning: An Introduction de Sutton y Barto.\n",
    "Según el problema original (y la documentación) el precipicio puede configurarse como resbaladizo (desactivado por defecto), por lo que el jugador puede moverse perpendicularmente a la dirección deseada en ocasiones (ver is_slippery). Sin embargo, no está disponible la opción en la versión v0 proporcionada por gymnasium, por lo que no usaremos la opción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2134f466-27e1-40d1-aaa5-93bfa56be60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "problema = gym.make(\"CliffWalking-v0\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48bec33-3001-4f94-a48f-2e2d9b1cfcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de espacio de estados Discrete(48)\n",
      "Estado aleatorio 45\n",
      "Hay 48  estados posibles.\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño de espacio de estados\", problema.observation_space)\n",
    "print(\"Estado aleatorio\", problema.observation_space.sample())\n",
    "size_estados = problema.observation_space.n\n",
    "print(\"Hay\", size_estados, \" estados posibles.\")\n",
    "problema.reset()\n",
    "problema.render()\n",
    "env=problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62bd78-87f5-4d59-991b-7f8f2936802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        #print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info,_  = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e6fca-40c5-4b9b-82e9-f5c9111fe195",
   "metadata": {},
   "source": [
    "¿Qué tal funciona la resolución con acciones aleatorias? \n",
    "\n",
    "Entrena un agente que resuelva el problema y comenta en detalle los resultados modificando los valores de los párametros, el número de episodios de aprendizaje, la recompensa por defecto. Define una métrica adecuada para el problema y evalua el aprendizaje para cada caso. \n",
    "Indica al final cual es la configuración de parámetros elegida. Al final del notebook se dan más detalles en **Actividad para Entregar**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea742c49-5990-453d-b3fb-6bd954dc80fe",
   "metadata": {},
   "source": [
    "## Actividad para entregar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d1521-7fbf-4ac6-b245-9dbd08af52f6",
   "metadata": {},
   "source": [
    "Configura el aprendizaje por refuerzo para resolver alguno/s de los problemas Toy Text de Gymnasium utilizando distintos valores de configuración de los parámetros y observa qué configuración se comporta mejor.\n",
    "     \n",
    "    1. Define las métricas adecuadas para poder evaluar qué tal se comporta el agente entrenado con distintas configuraciones de parámetros.\n",
    "    2. Realiza una variación dinámica de los valores de los parámetros durante el entrenamiento para que los valores no sean fijos \n",
    "    2. Modifica la recompensa por defecto para ver cómo afecta al entrenamiento. Observa cuál es la función de recompensa que se define por defecto (consulta en la documentación de gym). Mejórala reescribiendo el valor de reward y observa cómo afecta la mejora de la función de recompensa en el proceso de aprendizaje. Explica cómo has medido esta mejora.\n",
    "    4. Realiza pruebas y saca conclusiones claras justificadas con el resumen de resultados. Puedes usar gráficas.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e6881-e5ab-4072-bfea-1e1da1660599",
   "metadata": {},
   "source": [
    "### Apartado 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51eeaea-9419-480a-aee7-85c75241bca6",
   "metadata": {},
   "source": [
    "Vamos a evaluar el funcionamiento de nuestro programa en base a las siguientes métricas:\n",
    "1. Número de pasos promedio.\n",
    "2. Número de veces que se tira por el precipio en promedio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504b9ca-660a-4e04-82b3-9db57860e75d",
   "metadata": {},
   "source": [
    "### Apartado 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e07185-9be4-4564-b185-7840e4ecfabf",
   "metadata": {},
   "source": [
    "Para modificar dinámicamente los parámetros, aplicaremos la siguiente fórmula: sea x_0 cualquiera de nuestros parámetros iniciales (α, γ ó ε):\n",
    "x_n = x_{n-1} - val, donde val= x_0*1000/numEpisodios. De esta forma, iremos restando cada 1000 pasos una cantidad proporcional fija, de tal forma que si el número de episodios es mayor que 1000, acaban siendo 0 (en caso contrario, consideramos que no hay un número suficiente de episodios como para disminuir alguno de los parámetros)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c89ef9-7cea-48fe-9d8c-14077a0af194",
   "metadata": {},
   "source": [
    "Entrenamiento del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37a6e03-9054-43d9-93b7-47c9ad1193cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero se inicializa la Q-table a matrix of zeros:\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257bae7e-271f-4dde-8ca5-d83e9b380661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895407e6-2846-4876-be22-44d4aa75f39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 200\n",
      "Valor de alpha:0.1\n",
      "Valor de gamma:0.6\n",
      "Valor de epsilon:0.1\n",
      "Training finished.\n",
      "\n",
      "CPU times: total: 109 ms\n",
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "episodes = 270\n",
    "valAlpha = alpha*1000/episodes\n",
    "valGamma = gamma*1000/episodes\n",
    "valEpsilon = epsilon*1000/episodes\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "    state,_ = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info, _ = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -100:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "    #Modificamos el valor de los parámetros cada 1000 pasos.\n",
    "    if i%1000 == 0 and i < episodes:\n",
    "        alpha = alpha - valAlpha\n",
    "        gamma = gamma - valGamma\n",
    "        epsilon = epsilon - valEpsilon\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episodio: {i}\")\n",
    "        print(f\"Valor de alpha:{alpha}\")\n",
    "        print(f\"Valor de gamma:{gamma}\")\n",
    "        print(f\"Valor de epsilon:{epsilon}\")\n",
    "        \n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467b5220-49e3-4606-a9ef-921af7ed7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 0\n",
      "Episodio: 1\n",
      "Episodio: 2\n",
      "Resultados después de 3 episodios:\n",
      "Pasos promedio por episodio: 13.0\n",
      "Número de caídas al precipicio promedio por episodio: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 3\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(f\"Episodio: {i}\")\n",
    "    state,_ = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[int(state)])\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        if reward == -100:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    \n",
    "print(f\"Resultados después de {episodes} episodios:\")\n",
    "print(f\"Pasos promedio por episodio: {total_epochs / episodes}\")\n",
    "print(f\"Número de caídas al precipicio promedio por episodio: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754f615-812c-425e-8767-3bb5fdb99b1f",
   "metadata": {},
   "source": [
    "Como podemos observar, después de haberlo entrenado el muñeco llega a la posición objetivo en un promedio de 13 pasos, lo cual es óptimo. Esto se debe a que tiene que ir una casilla hacia arriba, 11 a la derecha y una hacia abajo. Además, de acuerdo con nuestra segunda métrica, observamos que en promedio no se cae ninguna vez por el precipicio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917569c5-6df1-454b-86f8-96c034a2edd5",
   "metadata": {},
   "source": [
    "### Apartado 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b820611-0feb-43a0-a662-858faaf448f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# Primero se inicializa la Q-table a matrix of zeros:\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "state, _ = env.reset()\n",
    "print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8da86f84-00d4-475a-873d-48a878f02e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio: 200\n",
      "Valor de alpha:0.1\n",
      "Valor de gamma:0.6\n",
      "Valor de epsilon:0.1\n",
      "Training finished.\n",
      "\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "episodes = 270\n",
    "valAlpha = alpha*1000/episodes\n",
    "valGamma = gamma*1000/episodes\n",
    "valEpsilon = epsilon*1000/episodes\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "    state,_ = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info, _ = env.step(action) \n",
    "\n",
    "        if reward == -100: #Si cae por el precipicio, modificamos la recompensa a -1\n",
    "            reward=-1\n",
    "            penalties += 1\n",
    "        if reward==0: #Si está en una celda normal, modificamos la recompensa a 11 para que se note el cambio de recompensa en el caso de caer por el precipicio.\n",
    "            reward=100\n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "    #Modificamos el valor de los parámetros cada 1000 pasos.\n",
    "    if i%1000 == 0 and i < episodes:\n",
    "        alpha = alpha - valAlpha\n",
    "        gamma = gamma - valGamma\n",
    "        epsilon = epsilon - valEpsilon\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episodio: {i}\")\n",
    "        print(f\"Valor de alpha:{alpha}\")\n",
    "        print(f\"Valor de gamma:{gamma}\")\n",
    "        print(f\"Valor de epsilon:{epsilon}\")\n",
    "        \n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6781f9e4-944f-4a26-8101-435ecb7e2133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados después de 3 episodios:\n",
      "Pasos promedio por episodio: 13.0\n",
      "Número de caídas al precipicio promedio por episodio: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 3\n",
    "\n",
    "for i in range(episodes):\n",
    "    state,_ = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[int(state)])\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        if reward == -100:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    \n",
    "print(f\"Resultados después de {episodes} episodios:\")\n",
    "print(f\"Pasos promedio por episodio: {total_epochs / episodes}\")\n",
    "print(f\"Número de caídas al precipicio promedio por episodio: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01308991-8804-41d9-ba37-6a5680ad0151",
   "metadata": {},
   "source": [
    "Observamos que, cambiando los episodios de entrenamiento a 270 y los de prueba a 3 nos da un promedio siempre de 13 pasos sin cambiar la recompensa. Sin embargo, en la versión en la que hemos modificado la recompensa, hemos probado a ejecutarlo varias veces, y a veces sale igual que en la versión sin modificar las recompensas, pero en otras se queda pillado al no haber aprendido lo suficiente y en otras nos sale que el número promedio de pasos es mayor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b213ac9-1d82-41ff-8b13-61ce9a663ec3",
   "metadata": {},
   "source": [
    "### Apartado 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112439e8-c54b-4bb5-be83-409f71f49b27",
   "metadata": {},
   "source": [
    "Hemos realizado diferentes pruebas y hemos observado que, una vez aprendido el camino óptimo (13 pasos), no vuelve a elegir otro camino ni se cae por el precipicio, debido a que siempre son el mismo origen y destino. Hemos reducido también el número de episodios de entrenamiento y hemos observado que en relativamente pocos episodios encuentra el camino adecuado y a partir de ese momento lo reproduce siempre. También hemos probado a modificar las recompensas y hemos visto que en la mayoría de los casos sigue haciendo el recorrido óptimo, pero alguna vez necesita más episodios de entrenamiento (en comparación con las recompensas originales). Por lo tanto, consideramos que en este problema no hay una diferencia significativa al modificar las recompensas porque no hay muchos parámetros a tener en cuenta, sólo evitar el precipicio y llegar al destino lo antes posibles. Con relación a la modificación dinámica de parámetros, tampoco notamos un impacto significativo debido a que, como hemos comentado anteriormente, se entrena relativamente rápido incluso sin modificación dinámica.\n",
    "En conclusión, al ser un problema en el que no se tienen en cuenta muchos parámetros, ninguna de las modificaciones realizadas es muy notoria."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
